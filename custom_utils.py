from sempy.fabric import get_workspace_id,get_lakehouse_id,list_workspaces,list_datasets, list_partitions
from sempy_labs import refresh_semantic_model
import pandas as pd
import ipywidgets as widgets
from IPython.display import display, clear_output, HTML
import uuid
import re
import warnings
warnings.filterwarnings("ignore")

#This function works with Python Runtime.
#Extracts Datasets and its tables/partitions from all the premium workspaces that the user has access too..
#For the first run set the load_from_lakehouse parameter to 'false', 

def get_dataset_details(load_from_lakehouse: bool,table_name: str,workspace_list: list = None) -> pd.DataFrame:
    """ Function to extract all Datasets and its tables and partitions of the executing user.

        load_from_lakehouse: set to False will extract the latest information across all the datasets.
        table_name: name of the lakehouse table. 
        workspace_list: ['Workspace1','Workspace2'], if left empty will extract all the accesible workspaces data
    """
    from deltalake import DeltaTable, write_deltalake
    workspace_id, lakehouse_id = get_workspace_id(), get_lakehouse_id()
    table_path = f"abfss://{workspace_id}@onelake.dfs.fabric.microsoft.com/{lakehouse_id}/Tables/{table_name}"
    if load_from_lakehouse: #Reads the existing data from the lakehouse
        try:
            delta = DeltaTable(table_path)
            result_df = delta.to_pyarrow_dataset().to_table().to_pandas()
            if workspace_list:
                result_df = delta_df[delta_df["Workspace Name"].isin(workspace_list)]
        except:
            result_df = pd.DataFrame()
            print('Please attach a lakehouse or Table with mentioned name not found, if running for the first time set load_from_lakehouse to False')
    else:
        premium_workspaces = list_workspaces(filter='isOnDedicatedCapacity eq true')
        if workspace_list:
            premium_workspaces = premium_workspaces[premium_workspaces["Name"].isin(workspace_list)]
        all_datasets = []
        dataset_partitions = []
        for idx1,row1 in premium_workspaces.iterrows():
            datasets = list_datasets(workspace=row1['Id'],mode='rest')[['Dataset Id','Dataset Name']]
            for idx2,row2 in datasets.iterrows():
                try:
                    partitions = list_partitions(workspace=row1['Id'],dataset=row2['Dataset Id'])[['Table Name','Partition Name']]
                    partitions = partitions[~partitions['Table Name'].str.startswith('LocalDateTable') & 
                                            ~partitions['Table Name'].str.startswith('DateTableTemplate')]
                                            #Add your own filters here if I had missed other scenarios..
                    partitions['Dataset Id'] = row2['Dataset Id']
                    dataset_partitions.append(partitions)
                except:
                    continue
            datasets['Workspace Id'] = row1['Id']
            datasets['Workspace Name'] = row1['Name']
            all_datasets.append(datasets)
        datasets_df = pd.concat(all_datasets,ignore_index=True)
        partitions_df = pd.concat(dataset_partitions,ignore_index=True)
        result_df = datasets_df.merge(partitions_df,on='Dataset Id', how='inner')
        write_deltalake(table_path, result_df, mode='overwrite') #write the dataframe to a lakehouse table
        print(f"Extracted {len(result_df['Dataset Id'].unique())} datasets info across {len(result_df['Workspace Id'].unique())} premium workspaces")
        print(f"{table_name} table saved to the Lakehouse")
    return result_df


# Defining a function to render app by accepting the input dataframe generated by the above function.
def launch_enhanced_refresh_ui(input_df):
    """
    Launches an interactive, enhanced UI for refreshing semantic models.
    This function creates a user interface for selecting workspaces, datasets, tables, and partitions from the provided DataFrame.
    It allows users to configure advanced refresh options such as refresh type, retry count, parallelism, commit mode, and policy application.
    The UI supports search and filtering for workspaces and datasets, and dynamically updates available options based on user selections.
    Upon triggering the refresh, the function leverages sempy under the hood to trace the refresh activities.
    Parameters
    ----------
    input_df : pandas.DataFrame
        A DataFrame containing at least the columns: "Workspace Name", "Workspace Id", "Dataset Name", "Dataset Id", "Table Name", and "Partition Name".
        This DataFrame is used to populate the dropdowns and selection widgets in the UI.
    Returns
    -------
    None
        Displays the interactive UI in a Notebook environment.
    """
    # Core Widgets
    workspace_search = widgets.Text(placeholder="Search workspaces...", layout=widgets.Layout(width="100%", margin='0 0 4px 0'))
    dataset_search = widgets.Text(placeholder="Search datasets...", layout=widgets.Layout(width="100%", margin='0 0 4px 0'))
    workspace_dd = widgets.Dropdown(options=sorted(input_df["Workspace Name"].dropna().unique()), layout=widgets.Layout(width="100%"))
    dataset_dd = widgets.Dropdown(options=[], layout=widgets.Layout(width="100%"))
    tables_ms = widgets.SelectMultiple(options=[], layout=widgets.Layout(width="100%", height="120px"))
    partitions_ms = widgets.SelectMultiple(options=[], layout=widgets.Layout(width="100%", height="120px"))
    # Advanced settings widgets
    adv_style = {"description_width": "140px"}
    adv_layout = widgets.Layout(width="360px")
    refresh_type_dd = widgets.Dropdown(options=["full", "clearValues", "calculate", "dataOnly", "automatic", "defragment"], value="full", description="Refresh Type:", style=adv_style, layout=adv_layout)
    retry_count_is = widgets.IntSlider(value=0, min=0, max=10, description="Retry Count:", style=adv_style, layout=adv_layout)
    apply_policy_cb = widgets.Checkbox(value=False, description="Apply Refresh Policy")
    max_parallelism_is = widgets.IntSlider(value=10, min=1, max=32, description="Max Parallelism:", style=adv_style, layout=adv_layout)
    commit_mode_dd = widgets.Dropdown(options=["transactional", "partialBatch"], value="transactional", description="Commit Mode:", style=adv_style, layout=adv_layout)
    visualize_cb = widgets.Checkbox(value=True, description="Visualize Refresh")
    refresh_btn = widgets.Button(description="Refresh", icon="fa-sync", button_style="warning", layout=widgets.Layout(width="150px", height="40px"))
    status_out = widgets.Output()
    #  Helpers
    def _get_id(column, **filters):
        query = " and ".join(f"`{k}` == '{v}'" for k, v in filters.items())
        ids = input_df.query(query)[column].dropna().unique()
        return ids[0] if len(ids) > 0 else None
    def _get_partition_pairs(ws, ds, tables):
        if not tables: return []
        subset = input_df.query("`Workspace Name` == @ws and `Dataset Name` == @ds and `Table Name` in @tables")[["Table Name", "Partition Name"]].dropna().drop_duplicates()
        return sorted([(f"{tbl} — {part}", f"'{tbl}'[{part}]") for tbl, part in subset.itertuples(index=False)])
    # Reactive Wiring
    # Store full lists for filtering
    all_workspaces = sorted(input_df["Workspace Name"].dropna().unique())
    def on_workspace_search(change):
        """Filter workspace dropdown based on search input."""
        search_term = change.new.lower()
        filtered_workspaces = [w for w in all_workspaces if search_term in w.lower()]
        current_val = workspace_dd.value
        workspace_dd.options = filtered_workspaces
        if current_val in filtered_workspaces:
            workspace_dd.value = current_val
    def on_dataset_search(change):
        """Filter dataset dropdown based on search input."""
        search_term = change.new.lower()
        ws = workspace_dd.value
        if not ws: return
        # Get the full list of datasets for the selected workspace
        all_datasets = sorted(input_df.loc[input_df["Workspace Name"] == ws, "Dataset Name"].dropna().unique())
        filtered_datasets = [d for d in all_datasets if search_term in d.lower()]
        current_val = dataset_dd.value
        dataset_dd.options = filtered_datasets
        if current_val in filtered_datasets:
            dataset_dd.value = current_val
    def on_workspace_change(*_):
        ws = workspace_dd.value
        # Reset dataset search and populate with full list for the new workspace
        dataset_search.value = '' 
        datasets = sorted(input_df.loc[input_df["Workspace Name"] == ws, "Dataset Name"].dropna().unique()) if ws else []
        dataset_dd.options = datasets
        dataset_dd.value = datasets[0] if datasets else None
    def on_dataset_change(*_):
        ws, ds = workspace_dd.value, dataset_dd.value
        tables = sorted(input_df.loc[(input_df["Workspace Name"] == ws) & (input_df["Dataset Name"] == ds), "Table Name"].dropna().unique()) if ws and ds else []
        tables_ms.options = tables
        tables_ms.value = partitions_ms.value = ()
        partitions_ms.options = []
    def on_tables_change(*_):
        partitions_ms.options = _get_partition_pairs(workspace_dd.value, dataset_dd.value, list(tables_ms.value))
        partitions_ms.value = ()
    # Observe changes on all interactive widgets
    workspace_search.observe(on_workspace_search, names="value")
    dataset_search.observe(on_dataset_search, names="value")
    workspace_dd.observe(on_workspace_change, names="value")
    dataset_dd.observe(on_dataset_change, names="value")
    tables_ms.observe(on_tables_change, names="value")
    #  Refresh Trigger
    def do_refresh(_):
        status_out.clear_output()
        with status_out:
            ws_name, ds_name = workspace_dd.value, dataset_dd.value
            if not ws_name or not ds_name:
                display(HTML("<div style='color:red; font-weight:bold;'>✖ Please select a workspace and dataset.</div>"))
                return
            if commit_mode_dd.value == "partialBatch" and apply_policy_cb.value:
                display(HTML("<div style='color:red; font-weight:bold;'>✖ For partialBatch commit mode, apply_refresh_policy must be False.</div>"))
                return
            ws_id = _get_id("Workspace Id", **{"Workspace Name": ws_name})
            ds_id = _get_id("Dataset Id", **{"Workspace Name": ws_name, "Dataset Name": ds_name})
            if not ws_id or not ds_id:
                display(HTML(f"<div style='color:red; font-weight:bold;'>✖ Could not resolve ID for <b>{ws_name}/{ds_name}</b>.</div>"))
                return
            params = { "workspace": ws_id, "dataset": ds_id, "tables": list(tables_ms.value) or None, "partitions": list(partitions_ms.value) or None,
                "refresh_type": refresh_type_dd.value,"retry_count": retry_count_is.value,"apply_refresh_policy": apply_policy_cb.value,
                "max_parallelism": max_parallelism_is.value,"commit_mode": commit_mode_dd.value,"visualize": visualize_cb.value }
            try:
                # This is the function that performs the refresh
                refresh_semantic_model(**params) 
            except Exception as e:
                display(HTML(f"<div style='color:red; font-weight:bold;'>✖ Refresh Failed: {e}</div>"))
    refresh_btn.on_click(do_refresh)
    on_workspace_change() # Initialize dataset dropdown
    # Helper function to create a labeled section with an optional search box
    def create_section(title, widget, hint=None, search_widget=None):
        items = [widgets.HTML(f"<div style='font-size:16px; font-weight:600; margin-bottom:4px;'>{title}</div>")]
        if hint:
            items.append(widgets.HTML(f"<div style='color:#757575; font-size:12px; margin-bottom:8px;'>{hint}</div>"))
        if search_widget:
            items.append(search_widget) # Add search box before the main widget
        items.append(widget)
        return widgets.VBox(items, layout=widgets.Layout(flex='1 1 50%', margin='0 8px'))
    #  App Assembly
    app = widgets.VBox([
        widgets.HTML(
        "<div style='font-size:20px; font-weight:bold; text-align:center; padding:14px; "
        "color:white; background:linear-gradient(90deg, #e68900, #ff9800); "
        "border-radius:12px;'>Enhanced Semantic Model Refresh</div>"),
        
        widgets.VBox([
            # Main content sections with search boxes
            widgets.HBox([
                # UPDATED: Pass the search widgets to the create_section function
                create_section("🗂️ Workspace", workspace_dd, search_widget=workspace_search),
                create_section("📅 Dataset", dataset_dd, search_widget=dataset_search)]),
            widgets.HTML("<hr style='border-top: 1px solid #e0e0e0; margin: 16px 0;'>"),
            widgets.HBox([
                create_section(
                    "𝄜 Tables <span style='color:gray; font-size:12px;'>(Hold Ctrl/Cmd to select multiple)</span>", 
                    tables_ms ),
                create_section(
                    "🧩 Partitions <span style='color:gray; font-size:12px;'>(Updates based on table selection)</span>", 
                    partitions_ms)] ),
            widgets.HTML("<hr style='border-top: 1px solid #e0e0e0; margin: 16px 0;'>"),
            widgets.Accordion(children=[
                widgets.HBox([
                    widgets.VBox([refresh_type_dd, retry_count_is, apply_policy_cb]),
                    widgets.VBox([max_parallelism_is, commit_mode_dd, visualize_cb])])], titles=("⚙️ Advanced Settings",)),
            # Refresh button
            widgets.HBox([refresh_btn], layout=widgets.Layout(justify_content="center", margin="20px 0 0 0"))], 
            layout=widgets.Layout(border='1px solid #e0e0e0', border_radius='16px', padding='24px', background_color='#fcfcfc', box_shadow='0 4px 12px rgba(0,0,0,0.05)')),
        # Status output
        status_out])
    return display(app)